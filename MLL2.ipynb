{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##IMPORT"
      ],
      "metadata": {
        "id": "rdwFBm5Qlg6G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ler6n4sadOeu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "# from matplotlib.patheffects import withSimplePatchShadow\n",
        "import os\n",
        "import cv2\n",
        "# from operator import le\n",
        "from sklearn.model_selection import train_test_split, KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fungsi Aktivasi"
      ],
      "metadata": {
        "id": "qi8Nm4ovlrWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(input):\n",
        "    return np.max(input,0)\n",
        "\n",
        "def sigmoid(input):\n",
        "    return 1/(1+np.exp(-input))"
      ],
      "metadata": {
        "id": "hFIXVBd0lc-W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer"
      ],
      "metadata": {
        "id": "MFWif3fslwic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_unit, activation):\n",
        "        self.n_unit = n_unit\n",
        "        self.activation = activation\n",
        "        self.bias = np.zeros(n_unit)\n",
        "        self.weight = []\n",
        "        self.deltaW = np.zeros(n_unit)\n",
        "\n",
        "    def init_weights(self, n_inputs):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.weight = np.random.randn(self.n_unit,n_inputs)\n",
        "        self.deltaW = np.zeros((self.n_unit))\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        if len(self.weight) == 0:\n",
        "           self.init_weights(len(inputs))\n",
        "        self.input = inputs\n",
        "        multisum = np.array([])\n",
        "        for i in range(self.n_unit):\n",
        "            multisum = np.append(multisum, \n",
        "            np.sum(np.multiply(self.weight[i], inputs)) + self.bias[i])\n",
        "\n",
        "        if self.activation == 'sigmoid':\n",
        "            matrixsigmoid = np.vectorize(sigmoid)\n",
        "            self.output = matrixsigmoid(multisum)\n",
        "        elif self.activation == 'linear':\n",
        "            self.output = multisum\n",
        "        else:\n",
        "            self.output = np.maximum(multisum, 0)\n",
        "        return self.output\n",
        "    \n",
        "    def d_sigmoid(self, inputs):\n",
        "        sigm = sigmoid(inputs)\n",
        "        return sigm * (1 - sigm)\n",
        "    \n",
        "    def d_relu(self,input):\n",
        "        return 1.0 if input >= 0 else 0\n",
        "\n",
        "    def d_act_funct(self,activation,inputs):\n",
        "        if (activation=='sigmoid'):\n",
        "            return self.d_sigmoid(inputs)\n",
        "        else:\n",
        "            return self.d_relu(inputs)\n",
        "\n",
        "    def backward(self,inputs):\n",
        "        derivative = np.array([])\n",
        "        for i in self.output:\n",
        "            derivative = np.append(derivative, self.d_act_funct(self.activation, i))\n",
        "        self.deltaW += np.multiply(derivative, inputs)\n",
        "        dE = np.matmul(inputs, self.weight)\n",
        "        return dE\n",
        "    \n",
        "    def update_weights(self, learn_rate, momentum):\n",
        "        for i in range(self.n_unit):\n",
        "            self.weight[i] = self.weight[i] - ((momentum * self.weight[i]) + (learn_rate * self.deltaW[i] * self.input))\n",
        "\n",
        "        self.bias = self.bias - ((momentum * self.bias) + (learn_rate * self.deltaW))\n",
        "        self.deltaW = np.zeros((self.n_unit))\n",
        "\n",
        "class ConvolutionLayer:\n",
        "    def __init__(self, nb_channel, nb_filter, filter_size, padding=0, stride=1):\n",
        "        self.nb_channel = nb_channel\n",
        "        self.nb_filter = nb_filter\n",
        "        self.filter_size = filter_size\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.weight = np.random.randn(nb_filter, nb_channel, filter_size, filter_size)\n",
        "        self.bias = np.zeros((nb_filter))\n",
        "        self.dw = np.zeros((nb_filter, nb_channel, filter_size, filter_size))\n",
        "        self.db = np.zeros((nb_filter))\n",
        "    \n",
        "    def add_zero_padding(self, inputs):\n",
        "        w_pad = inputs.shape[1] + self.padding * 2\n",
        "        h_pad = inputs.shape[2] + self.padding * 2\n",
        "        \n",
        "        inputs_padded = np.zeros((inputs.shape[0], w_pad, h_pad))\n",
        "        for s in range(inputs.shape[0]):\n",
        "            inputs_padded[s, self.padding:w_pad-self.padding, self.padding:h_pad-self.padding] = inputs[s, :, :]\n",
        "\n",
        "        return inputs_padded\n",
        "    \n",
        "    def update_weights(self, learn_rate, momentum):\n",
        "        self.weight -= learn_rate * self.dw\n",
        "        self.bias -= learn_rate * self.db\n",
        "\n",
        "        # Reset error\n",
        "        self.dw = np.zeros((self.nb_filter, self.nb_channel, self.filter_size, self.filter_size))\n",
        "        self.db = np.zeros((self.nb_filter))\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        w, h = inputs.shape[1], inputs.shape[2]\n",
        "        v_w = int((w - self.filter_size + 2*self.padding)/self.stride + 1)\n",
        "        v_h = int((h - self.filter_size + 2*self.padding)/self.stride + 1)\n",
        "        \n",
        "        self.inputs = self.add_zero_padding(inputs)\n",
        "        featureMap = np.zeros((self.nb_filter, v_w, v_h))\n",
        "\n",
        "        for k in range(self.nb_filter):\n",
        "            for i in range(v_w):\n",
        "                for j in range(v_h):\n",
        "                    recField = self.inputs[:, i:i+self.filter_size, j:j+self.filter_size]\n",
        "                    featureMap[k, i, j] = np.sum(recField * self.weight[k, :, :, :] + self.bias[k])\n",
        "        \n",
        "        return self.detector(featureMap)\n",
        "\n",
        "    def backward(self, prev_errors):\n",
        "        dx = np.zeros(self.inputs.shape)\n",
        "        dw = np.zeros(self.weight.shape)\n",
        "        db = np.zeros(self.bias.shape)\n",
        "\n",
        "        f, w, h = prev_errors.shape\n",
        "\n",
        "        for k in range(f):\n",
        "            db[k] = np.sum(prev_errors[k, :, :])\n",
        "        \n",
        "        for k in range(f):\n",
        "            for i in range(w):\n",
        "                for j in range(h):\n",
        "                    dw[k, :, :, :] += prev_errors[k, i, j] * self.inputs[:, i:i+self.filter_size, j:j+self.filter_size]\n",
        "                    dx[:, i:i+self.filter_size, j:j+self.filter_size] += prev_errors[k, i, j] * self.weight[k, :, :, :]\n",
        "\n",
        "        self.dw += dw\n",
        "        self.db += db\n",
        "        \n",
        "        return self.detector(dx)\n",
        "\n",
        "    def detector(self,input):\n",
        "        return np.maximum(input, 0)\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self,filter_size,stride_size,mode):\n",
        "        self.filter_size = filter_size\n",
        "        self.stride_size = stride_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        depth = inputs.shape[0]\n",
        "        length = ((inputs.shape[1] - self.filter_size) // self.stride_size) + 1\n",
        "        pooled_map = np.zeros([depth, length, length], dtype=np.double)\n",
        "\n",
        "        for d in range(0, depth):\n",
        "            for w in range(0, length):\n",
        "                for h in range(0, length):\n",
        "                    if (self.mode.lower() == 'average'):\n",
        "                        pooled_map[d,w,h] = self.average(inputs,d,w,h)\n",
        "                    elif (self.mode.lower() == 'max'):\n",
        "                        pooled_map[d,w,h] = self.max(inputs,d,w,h)\n",
        "        return pooled_map\n",
        "    \n",
        "    def average(self,inputs,d,r_pos,b_pos):\n",
        "        return np.average(inputs[d, \n",
        "                                r_pos*self.filter_size:(r_pos*self.filter_size + self.filter_size),\n",
        "                                b_pos*self.filter_size:(b_pos*self.filter_size + self.filter_size)\n",
        "                                ])\n",
        "\n",
        "    def max(self,inputs,d,r_pos,b_pos):\n",
        "        return np.max(inputs[d, \n",
        "                                r_pos*self.filter_size:(r_pos*self.filter_size + self.filter_size),\n",
        "                                b_pos*self.filter_size:(b_pos*self.filter_size + self.filter_size)\n",
        "                                ])\n",
        "\n",
        "    def backward(self, prev_errors):\n",
        "        F, W, H = self.input.shape\n",
        "        dx = np.zeros(self.input.shape)\n",
        "        for i in range(0, F):\n",
        "            for j in range(0, W, self.filter_size):\n",
        "                for k in range(0, H, self.filter_size):\n",
        "                    st = np.argmax(self.input[i, j : j + self.filter_size, k : k + self.filter_size])\n",
        "                    (idx, idy) = np.unravel_index(st, (self.filter_size, self.filter_size))\n",
        "                    if ((j + idx) < W and (k + idy) < H):\n",
        "                        dx[i, j + idx, k + idy] = prev_errors[i, int(j / self.filter_size) % prev_errors.shape[1], int(k / self.filter_size) % prev_errors.shape[2]]\n",
        "        return dx\n",
        "    \n",
        "    def update_weights(self, learn_rate, momentum):\n",
        "        # Todo\n",
        "        pass\n",
        "\n",
        "class FlattenLayer:\n",
        "    def init(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.channel, self.width, self.height = inputs.shape\n",
        "        output = inputs.flatten()\n",
        "        return output\n",
        "    \n",
        "    def backward(self, inputs):\n",
        "        return inputs.reshape(self.channel, self.width, self.height)\n",
        "\n",
        "    def update_weights(self, learning_rate, momentum):\n",
        "        pass\n",
        "\n",
        "class LSTMLayer:\n",
        "    def __init__(self, input_size, n_cells):\n",
        "        self.input_size = input_size\n",
        "        self.n_cells = n_cells\n",
        "\n",
        "        self.c_prev = np.zeros((n_cells,1))\n",
        "        self.h_prev = np.zeros((n_cells,1))\n",
        "\n",
        "        self.sigmoid = np.vectorize(sigmoid) # allow function to receive input in form of vector\n",
        "\n",
        "        #parameternya dijadiin class\n",
        "        self.input_param = self.LSTMParameter(self.input_size, self.n_cells)\n",
        "        self.cell_param = self.LSTMParameter(self.input_size, self.n_cells)\n",
        "        self.forget_param = self.LSTMParameter(self.input_size, self.n_cells)\n",
        "        self.output_param = self.LSTMParameter(self.input_size, self.n_cells)\n",
        "        self.training_param = {}\n",
        "    \n",
        "    class LSTMParameter:\n",
        "        def __init__(self,size,n_cells):\n",
        "            self.u = np.random.rand(n_cells, size)\n",
        "            self.w = np.random.rand(n_cells)\n",
        "            self.b = np.random.rand(n_cells)\n",
        "\n",
        "\n",
        "    def forgetGate(self, timestep):\n",
        "        self.training_param['f'+str(timestep)] = self.sigmoid(\n",
        "                np.dot(self.forget_param.u, self.x[timestep]) + \n",
        "                np.dot(self.forget_param.w, self.h_prev) + \n",
        "                self.forget_param.b\n",
        "            )\n",
        "\n",
        "    def inputGate(self,timestep):\n",
        "        self.training_param['i' + str(timestep)] = self.sigmoid(\n",
        "            np.matmul(self.input_param.u,self.x[timestep]) \n",
        "            + np.multiply(self.input_param.w,self.h_prev[timestep]) \n",
        "            + self.input_param.b)\n",
        "    \n",
        "    def cellState(self,timestep):\n",
        "        self.training_param['Caccent'+str(timestep)] = np.tanh(\n",
        "            np.dot(self.cell_param.u, self.x[timestep]) + np.dot(self.cell_param.w, self.h_prev) + self.cell_param.b)\n",
        "        self.training_param['C'+str(timestep)] = (np.multiply(\n",
        "            self.training_param['f'+str(timestep)], self.c_prev) + \n",
        "            np.multiply(self.training_param['i'+str(timestep)], self.training_param['Caccent'+str(timestep)]))\n",
        "\n",
        "    def outputGate(self, timestep):\n",
        "        self.training_param['o'+str(timestep)] = self.sigmoid(\n",
        "                np.dot(self.output_param.u, self.x[timestep]) + \n",
        "                np.dot(self.output_param.w, self.h_prev) + \n",
        "                self.output_param.b\n",
        "            )\n",
        "        self.training_param['h'+str(timestep)] = np.multiply(self.training_param['o'+str(timestep)], np.tanh(self.training_param['C'+str(timestep)]))\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        self.x = inputs\n",
        "        for i in range(self.n_cells):\n",
        "            self.forgetGate(i)\n",
        "            self.inputGate(i)\n",
        "            self.cellState(i)\n",
        "            self.outputGate(i)\n",
        "            self.c_prev = self.training_param['C'+str(i)]\n",
        "            self.h_prev = self.training_param['h'+str(i)]\n",
        "        \n",
        "        output = self.training_param['h'+str(self.n_cells-1)]\n",
        "        return output\n",
        "\n",
        "    def backward(self, inputs):\n",
        "        # engga ada di spek\n",
        "        pass\n",
        "\n",
        "    def update_weights(self, learning_rate, momentum):\n",
        "        # engga ada di spek\n",
        "        pass"
      ],
      "metadata": {
        "id": "LYsPNTWydeOO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "_143zMQNoaxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn import metrics\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, layers=[]):\n",
        "        self.layers = layers\n",
        "    \n",
        "    def save_model(self, namefile):\n",
        "        with open(namefile, 'wb') as f:\n",
        "            pickle.dump(self.layers,  f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    def load_model(self,namefile):\n",
        "        with open(namefile, 'rb') as f:\n",
        "            temp = pickle.load(f)\n",
        "        self.layers = temp.copy()\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        out = inputs.copy()\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "    \n",
        "    def _forward(self,inputs):\n",
        "        out = inputs.copy()\n",
        "        result = [out]\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "            result.append(out)\n",
        "\n",
        "        return result\n",
        "    \n",
        "    def fit(self,features, target, batch_size, epoch, learn_rate, momentum=1):\n",
        "        y = np.array([])\n",
        "        output = np.array([])\n",
        "\n",
        "        for e in range(epoch):\n",
        "            print(\"Epoch: \", e+1)\n",
        "            sum_loss = 0\n",
        "            \n",
        "            for b in range(batch_size):\n",
        "                current_idx = (batch_size * e + b) % len(features)\n",
        "                res = self._forward(features[current_idx])\n",
        "                current_output = res[len(res) - 1][0]\n",
        "                current_y = target[current_idx]\n",
        "                \n",
        "                dE = np.array([current_y - current_output]) * -1\n",
        "                for i in reversed(range(len(self.layers))):\n",
        "                    dE = self.layers[i].backward(dE)\n",
        "                sum_loss += 0.5 * (current_y - current_output)**2\n",
        "                \n",
        "                y = np.append(y, current_y)\n",
        "                output = np.rint(np.append(output, current_output))\n",
        "\n",
        "            for l in reversed(range(len(self.layers))):\n",
        "                self.layers[l].update_weights(learn_rate, momentum)\n",
        "            avg_loss = sum_loss/batch_size\n",
        "            \n",
        "            print(\"Loss: \", avg_loss)\n",
        "            print(\"Accuracy: \", metrics.accuracy_score(y, output))\n",
        "\n",
        "class ModelLSTM:\n",
        "\n",
        "    def __init__(self, layers=[]):\n",
        "        self.layers = layers\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        out = inputs.copy()\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "        return out\n",
        "    \n",
        "    def _forward(self,inputs):\n",
        "        out = inputs.copy()\n",
        "        result = [out]\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "            result.append(out)\n",
        "        return result\n",
        "    \n",
        "    def fit(self,features):\n",
        "        return self.forward(features)\n"
      ],
      "metadata": {
        "id": "xO6k1dZOdhMx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tubes 1 CNN"
      ],
      "metadata": {
        "id": "l7c3U3H8mALe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Image"
      ],
      "metadata": {
        "id": "wyjlFHXUmGLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to read image to numpy array of file name\n",
        "def read_dataset(dataset_path):\n",
        "    folder_list = os.listdir(dataset_path)\n",
        "    folder_list = sorted(folder_list)\n",
        "    folder_path = []; class_label = np.ndarray(shape=(0)); class_dictionary = {}\n",
        "    \n",
        "    for i in range(len(folder_list)):#loop for all class folders\n",
        "        class_folder_path = os.path.join(dataset_path, folder_list[i])\n",
        "        list_image_name = os.listdir(class_folder_path)\n",
        "        list_image_name = sorted(list_image_name)\n",
        "        temp_folder_path = [os.path.join(class_folder_path, j) for j in list_image_name]\n",
        "        folder_path += temp_folder_path\n",
        "        temp_class_label = np.full((len(list_image_name)),np.int16(i))\n",
        "        class_label = np.concatenate((class_label, temp_class_label), axis=0)\n",
        "        class_dictionary[str(i)] = folder_list[i]\n",
        "    return np.asarray(folder_path), class_label, class_dictionary\n",
        "\n",
        "# function to read all images in a folder to numpy array of image matrix\n",
        "def list_img_to_matrix(folder_path, size = (400, 400)):\n",
        "    list_of_image_matrix = []\n",
        "    for file_img in folder_path:\n",
        "        image = cv2.imread(file_img, 1)\n",
        "\n",
        "        image_matrix = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_matrix = cv2.resize(image_matrix, size)\n",
        "        image_matrix = np.array(image_matrix)\n",
        "\n",
        "        list_of_image_matrix.append(image_matrix)\n",
        "    \n",
        "    list_of_image_matrix = np.array(list_of_image_matrix, dtype=\"object\")\n",
        "    list_of_image_matrix = np.transpose(list_of_image_matrix, (0,3,1,2))\n",
        "    return list_of_image_matrix\n"
      ],
      "metadata": {
        "id": "_YbZtc3mdcl2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download dataset\n",
        "!gdown 1NgRAihCd-2B0YsFOtF1r8VaSBwJJHBaT\n",
        "!!unzip /content/DATASET_TUBES1.zip -d test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BJ08Vnbmpa5",
        "outputId": "ed932d4c-58a2-455e-8846-991a73530154"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NgRAihCd-2B0YsFOtF1r8VaSBwJJHBaT\n",
            "To: /content/DATASET_TUBES1.zip\n",
            "\r  0% 0.00/888k [00:00<?, ?B/s]\r100% 888k/888k [00:00<00:00, 147MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  /content/DATASET_TUBES1.zip',\n",
              " '   creating: test/cats/',\n",
              " '  inflating: test/cats/cat.0.jpg     ',\n",
              " '  inflating: test/cats/cat.15.jpg    ',\n",
              " '  inflating: test/cats/cat.17.jpg    ',\n",
              " '  inflating: test/cats/cat.19.jpg    ',\n",
              " '  inflating: test/cats/cat.2.jpg     ',\n",
              " '  inflating: test/cats/cat.21.jpg    ',\n",
              " '  inflating: test/cats/cat.23.jpg    ',\n",
              " '  inflating: test/cats/cat.26.jpg    ',\n",
              " '  inflating: test/cats/cat.36.jpg    ',\n",
              " '  inflating: test/cats/cat.38.jpg    ',\n",
              " '  inflating: test/cats/cat.40.jpg    ',\n",
              " '  inflating: test/cats/cat.45.jpg    ',\n",
              " '  inflating: test/cats/cat.48.jpg    ',\n",
              " '  inflating: test/cats/cat.49.jpg    ',\n",
              " '  inflating: test/cats/cat.50.jpg    ',\n",
              " '  inflating: test/cats/cat.58.jpg    ',\n",
              " '  inflating: test/cats/cat.60.jpg    ',\n",
              " '  inflating: test/cats/cat.61.jpg    ',\n",
              " '  inflating: test/cats/cat.71.jpg    ',\n",
              " '  inflating: test/cats/cat.9.jpg     ',\n",
              " '   creating: test/dogs/',\n",
              " '  inflating: test/dogs/dog.0.jpg     ',\n",
              " '  inflating: test/dogs/dog.24.jpg    ',\n",
              " '  inflating: test/dogs/dog.25.jpg    ',\n",
              " '  inflating: test/dogs/dog.3.jpg     ',\n",
              " '  inflating: test/dogs/dog.30.jpg    ',\n",
              " '  inflating: test/dogs/dog.34.jpg    ',\n",
              " '  inflating: test/dogs/dog.36.jpg    ',\n",
              " '  inflating: test/dogs/dog.38.jpg    ',\n",
              " '  inflating: test/dogs/dog.4.jpg     ',\n",
              " '  inflating: test/dogs/dog.40.jpg    ',\n",
              " '  inflating: test/dogs/dog.42.jpg    ',\n",
              " '  inflating: test/dogs/dog.5.jpg     ',\n",
              " '  inflating: test/dogs/dog.50.jpg    ',\n",
              " '  inflating: test/dogs/dog.51.jpg    ',\n",
              " '  inflating: test/dogs/dog.58.jpg    ',\n",
              " '  inflating: test/dogs/dog.60.jpg    ',\n",
              " '  inflating: test/dogs/dog.61.jpg    ',\n",
              " '  inflating: test/dogs/dog.63.jpg    ',\n",
              " '  inflating: test/dogs/dog.8.jpg     ',\n",
              " '  inflating: test/dogs/dog.9.jpg     ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/test\"\n",
        "\n",
        "dataset_path = DATA_PATH\n",
        "folder_path, class_label, class_dictionary = read_dataset(dataset_path)\n",
        "Layers = [\n",
        "    ConvolutionLayer(3,10,2,0,2),\n",
        "    Pooling(2,2,\"max\"),\n",
        "    FlattenLayer(),\n",
        "    DenseLayer(2,\"relu\"),\n",
        "    DenseLayer(4,\"relu\"),\n",
        "    DenseLayer(1,\"sigmoid\")\n",
        "]\n",
        "\n",
        "matrix_images = list_img_to_matrix(folder_path)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(matrix_images, class_label, test_size=0.1)\n",
        "\n",
        "cnn = Model(Layers)\n",
        "cnn.fit(features=X_train, target=y_train, batch_size=5, epoch=3, learn_rate=0.1)\n",
        "\n",
        "filename = \"model1\"\n",
        "cnn.save_model(filename)\n",
        "\n",
        "cnn2 = Model()\n",
        "cnn2.load_model(filename)\n",
        "\n",
        "output = np.array([])\n",
        "for data in X_test:\n",
        "    forward_cnn = cnn2.forward(data)\n",
        "    output = np.append(output, np.rint(forward_cnn))\n",
        "\n",
        "print(\"\\nPredicted:\", output)\n",
        "print(\"\\nAccuracy:\", metrics.accuracy_score(y_test, output))\n",
        "print(\"\\nConfusion matrix:\\n\", metrics.confusion_matrix(y_test, output))\n",
        "\n",
        "kf = KFold(n_splits=10,shuffle=True)\n",
        "best_accuracy = 0\n",
        "best_model = None\n",
        "for train_index, test_index in kf.split(matrix_images):\n",
        "    X_train, X_test = matrix_images[train_index], matrix_images[test_index]\n",
        "    y_train, y_test = class_label[train_index], class_label[test_index]\n",
        "\n",
        "    cnnKfold = Model(layers=Layers)\n",
        "    cnnKfold.fit(features=X_train, target=y_train, batch_size=5, epoch=3, learn_rate=0.1)\n",
        "    output = np.array([])\n",
        "    for data in X_test:\n",
        "        forward_cnn = cnnKfold.forward(data)\n",
        "        output = np.append(output, np.rint(forward_cnn))\n",
        "    \n",
        "    accuracy = metrics.accuracy_score(y_test, output)\n",
        "    print(\"\\nAccuracy:\", accuracy)\n",
        "    print(\"Confusion matrix:\\n\", metrics.confusion_matrix(y_test, output))\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "    \n",
        "print(\"\\nBest Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU1BSjXndfUt",
        "outputId": "4bfa25e4-b669-4949-e31c-130914aacd34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.25\n",
            "Accuracy:  0.2\n",
            "Epoch:  2\n",
            "Loss:  0.1306787176146603\n",
            "Accuracy:  0.2\n",
            "Epoch:  3\n",
            "Loss:  0.1259741615752928\n",
            "Accuracy:  0.26666666666666666\n",
            "\n",
            "Predicted: [1. 1. 1. 1.]\n",
            "\n",
            "Accuracy: 0.0\n",
            "\n",
            "Confusion matrix:\n",
            " [[0 4]\n",
            " [0 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.12661428005104558\n",
            "Accuracy:  0.0\n",
            "Epoch:  2\n",
            "Loss:  0.11772587648905235\n",
            "Accuracy:  0.5\n",
            "Epoch:  3\n",
            "Loss:  0.11795177231696716\n",
            "Accuracy:  0.6666666666666666\n",
            "\n",
            "Accuracy: 0.25\n",
            "Confusion matrix:\n",
            " [[1 0]\n",
            " [3 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794588491276932\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603825290602\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603425902409\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.25\n",
            "Confusion matrix:\n",
            " [[1 0]\n",
            " [3 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436304825\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.1179460343603389\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.1179460343604094\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.75\n",
            "Confusion matrix:\n",
            " [[3 0]\n",
            " [1 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040756\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.5\n",
            "Confusion matrix:\n",
            " [[2 0]\n",
            " [2 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.75\n",
            "Confusion matrix:\n",
            " [[3 0]\n",
            " [1 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.25\n",
            "Confusion matrix:\n",
            " [[1 0]\n",
            " [3 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.75\n",
            "Confusion matrix:\n",
            " [[3 0]\n",
            " [1 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.25\n",
            "Confusion matrix:\n",
            " [[1 0]\n",
            " [3 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.75\n",
            "Confusion matrix:\n",
            " [[3 0]\n",
            " [1 0]]\n",
            "Epoch:  1\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  2\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "Epoch:  3\n",
            "Loss:  0.11794603436040763\n",
            "Accuracy:  1.0\n",
            "\n",
            "Accuracy: 0.5\n",
            "Confusion matrix:\n",
            " [[2 0]\n",
            " [2 0]]\n",
            "\n",
            "Best Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TUBES 2"
      ],
      "metadata": {
        "id": "APRV4qXCojMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1b_C6n6XcDwtQ8nmC-dUGmyAw2B2DrShV\n",
        "!gdown 1uqVXJcLlCssqIqtR85unU2Z6uiBxqZNT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qQ6a4ATeijE",
        "outputId": "f550d64d-d1ea-4965-9ef0-ff9a20cf2811"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1b_C6n6XcDwtQ8nmC-dUGmyAw2B2DrShV\n",
            "To: /content/ETH-USD-Train.csv\n",
            "100% 191k/191k [00:00<00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uqVXJcLlCssqIqtR85unU2Z6uiBxqZNT\n",
            "To: /content/ETH-USD-Test.csv\n",
            "100% 5.60k/5.60k [00:00<00:00, 10.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train_path = '/content/ETH-USD-Train.csv'\n",
        "test_path = '/content/ETH-USD-Test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_path, infer_datetime_format=True)\n",
        "df_test = pd.read_csv(test_path, infer_datetime_format=True)\n",
        "df_train['Date'] = pd.to_datetime(df_train.Date)\n",
        "dataset = df_train.loc[:, ['Close']].values\n",
        "dataset = dataset.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "data_scaled = scaler.fit_transform(dataset)\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "time_step = 32\n",
        "for i in range(len(data_scaled) - time_step - 1):\n",
        "    a = data_scaled[i:(i + time_step), 0]\n",
        "    X_train.append(a)\n",
        "    y_train.append(data_scaled[i + time_step, 0])\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "print(\"trainX shape: {}\\ntrainY shape: {}\". format(X_train.shape, y_train.shape))\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGic6Duedy6c",
        "outputId": "7700f82c-3197-45c6-f288-e3680cf4f5c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX shape: (1717, 32)\n",
            "trainY shape: (1717,)\n",
            "[0.0500395  0.0454642  0.04872747 0.04729487 0.0491579  0.05358176\n",
            " 0.05267774 0.05216312 0.05247405 0.05569289 0.0571257  0.05973666\n",
            " 0.05839797 0.06268138 0.06892405 0.08261865 0.08079221 0.0818612\n",
            " 0.08377014 0.08219371 0.07259534 0.07673914 0.08084805 0.08019425\n",
            " 0.08070273 0.08162304 0.08015872 0.0728206  0.07405162 0.07862523\n",
            " 0.08232062 0.07559844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = [\n",
        "    LSTMLayer(32, 256),\n",
        "    DenseLayer(1,\"linear\")\n",
        "]\n",
        "model = ModelLSTM(layer)\n",
        "model.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdAa0gatfWRc",
        "outputId": "afe87ad2-1dc9-429b-8798-0c9de0735e5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([860.70039794])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}